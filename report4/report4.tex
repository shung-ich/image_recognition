\documentclass[a4j, titlepage]{jarticle}
\usepackage[dvipdfmx]{graphicx}
\usepackage{ascmac}

\usepackage{listings,jlisting}

\lstset{%
  language={C},
  basicstyle={\small},%
  identifierstyle={\small},%
  commentstyle={\small\itshape},%
  keywordstyle={\small\bfseries},%
  ndkeywordstyle={\small},%
  stringstyle={\small\ttfamily},
  frame={tb},
  breaklines=true,
  columns=[l]{fullflexible},%
  numbers=left,%
  xrightmargin=0zw,%
  xleftmargin=3zw,%
  numberstyle={\scriptsize},%
  stepnumber=1,
  numbersep=1zw,%
  lineskip=-0.5ex%
}

\begin{document}

\title{計算機科学実験及演習4 画像認識　\\ \bf レポート4}
% ↓ここに自分の氏名を記入
\author{高橋駿一　2018年度入学 1029-30-3949}
\西暦
\date{提出日: \today} % コンパイル時の日付が自動で挿入される
\maketitle

\clearpage

\section*{課題4}
\subsection*{課題内容}
MNISTのテスト画像1枚を入力とし, 3層ニューラルネットワークを用いて, 0～9の値のうち1つを出力するプログラムを作成した.

\subsection*{作成したプログラムの説明}
機能ごとにクラスや関数を実装している.
    \subsubsection*{1. paramsクラス}
        \begin{lstlisting}[caption=パラメータWとbに関する処理 ,label=fuga]
        class params:
            def __init__(self, M, d):
                np.random.seed(seed=32)
                self.W = np.random.normal(0, 1/d, (d, M))
                self.b = np.random.normal(0, 1/d, (1, M))
                self.eta = 0.01
                approach = 'Adam'
                self.op1 = optimize(approach)
                self.op2 = optimize(approach)

            def update(self, dW, db):
                self.W += self.op1.update(dW)
                self.b += self.op2.update(db)

            def save(self, i):
                np.save('./w{}'.format(i), self.W)
                np.save('./b{}'.format(i), self.b)
        \end{lstlisting}
        重みWとバイアスbの初期化, 更新, 保存を行うクラスである.
        \paragraph*{\_\_init\_\_(コンストラクタ)}
            \subparagraph*{入力}
            \begin{itemize}
                \item M: 次の層のノード数
                \item d: 前の層のノード数
            \end{itemize}
            \subparagraph*{メンバ変数}
            \begin{itemize}
                \item W: 重み
                \item b: バイアス
                \item eta: 学習率
                \item op1, op2: W, bの最適化を行うクラスのインスタンス
            \end{itemize}
            W, bをM, dに基づいて初期化し, それぞれを最適化するためのインスタンスを作成する.
        \paragraph*{update関数}
            \subparagraph*{入力}
            \begin{itemize}
                \item dW: \(\displaystyle \frac{\partial E_n}{\partial W}\)
                \item db: \(\displaystyle \frac{\partial E_n}{\partial b}\)
            \end{itemize}
            $E_n$: 損失関数である.
            op1, op2によってW, bの値を更新する.
        \paragraph*{save関数}
            \subparagraph*{入力}
            \begin{itemize}
                \item i: 文字列
            \end{itemize}
            入力iを受けてW, bをそれぞれwi, biという名前の.npyファイルで保存する.

    \subsubsection*{2. optimizeクラス}
        \begin{lstlisting}[caption=パラメータ更新の手法選択,label=fuga]
        class optimize:
            def __init__(self, approach):
                self.approach = approach
                self.diff = 0
                if approach == 'default':
                    self.eta = 0.01
                elif approach == 'SGD':
                    self.eta = 0.01
                    self.alpha = 0.9
                elif approach == 'AdaGrad':
                    self.h = 1e-8
                    self.eta = 0.001
                elif approach == 'RMSProp':
                    self.h = 0
                    self.eta = 0.001
                    self.rho = 0.9
                    self.epsilon = 1e-8
                elif approach == 'AdaDelta':
                    self.h = 0
                    self.s = 0
                    self.rho = 0.95
                    self.epsilon = 1e-6
                elif approach == 'Adam':
                    self.t = 0
                    self.m = 0
                    self.v = 0
                    self.alpha = 0.001
                    self.beta1 = 0.9
                    self.beta2 = 0.999
                    self.epsilon = 1e-8

            def update(self, d_):
                if self.approach == 'default':
                    self.diff = (-1) * self.eta * d_
                elif self.approach == 'SGD':
                    self.diff = self.alpha * self.diff - self.eta * d_
                elif self.approach == 'AdaGrad':
                    self.h = self.h + d_ * d_
                    self.diff = (-1) * self.eta / np.sqrt(self.h) * d_
                elif self.approach == 'RMSProp':
                    self.h = self.rho * self.h + (1 - self.rho) * d_ * d_
                    self.diff = (-1) * self.eta / (np.sqrt(self.h) + self.epsilon) * d_
                elif self.approach == 'AdaDelta':
                    self.h = self.rho * self.h + (1 - self.rho) * d_ * d_
                    self.diff = (-1) * np.sqrt(self.s + self.epsilon) / np.sqrt(self.h + self.epsilon) * d_
                    self.s = self.rho * self.s + (1 - self.rho) * self.diff * self.diff
                elif self.approach == 'Adam':
                    self.t = self.t + 1
                    self.m = self.beta1 * self.m + (1 - self.beta1) * d_
                    self.v = self.beta2 * self.v + (1 - self.beta2) * d_ * d_
                    m_hat = self.m / (1 - self.beta1 ** self.t)
                    v_hat = self.v / (1 - self.beta2 ** self.t)
                    self.diff = (-1) * self.alpha * m_hat / (np.sqrt(v_hat) + self.epsilon)
                return self.diff
        \end{lstlisting}
        パラメータ更新の手法を選択するためのクラスである.
        \paragraph*{\_\_init\_\_(コンストラクタ)}
            \subparagraph*{入力}
            \begin{itemize}
                \item approach: 最適化の手法を表す文字列
            \end{itemize}
            \subparagraph*{メンバ変数(共通)}
            \begin{itemize}
                \item approach: 最適化の手法を表す文字列
                \item diff: パラメータの更新前後の差分
            \end{itemize}
            入力approachを元にパラメータ更新の手法を選択し, それぞれの計算に必要な変数をメンバ変数を初期化する.
        \paragraph*{update関数}
            \subparagraph*{入力}
            \begin{itemize}
                \item d\_: \(\displaystyle \frac{\partial E_n}{\partial W}\)
            \end{itemize}
            メンバ変数approachによって更新手法を制御し, それぞれのメンバ変数と入力d\_を元にdiffを計算する.


    \subsubsection*{3. load関数}
        \begin{lstlisting}[caption=Wとbの.npyファイルを読み込み,label=fuga]
        def load(i):
            W_loaded = np.load('./w{}.npy'.format(i))
            b_loaded = np.load('./b{}.npy'.format(i))
            return W_loaded, b_loaded
        \end{lstlisting}
        .npyファイルを読み込むための関数である.
            \subparagraph*{入力}
            \begin{itemize}
                \item i: 文字列
            \end{itemize}
            入力iを受けてwi.npy, bi.npyを読み込む.

    \subsubsection*{4. create\_batch関数}
        \begin{lstlisting}[caption=ミニバッチを作成,label=fuga]
        def create_batch(X):
            batch_size = 100
            np.random.seed(seed=32)
            batch_index = np.random.choice(len(X), (600, batch_size))
            return batch_index
        \end{lstlisting}
        ミニバッチのインデックスを作成するための関数である.
            \subparagraph*{入力}
            \begin{itemize}
                \item X: 元の画像データ
            \end{itemize}
            シードを固定し, 0以上入力Xの長さ(画像データの数)以下の非負整数を600*batch\_sizeの行列に割り当てる.

    \subsubsection*{5. input\_layer\_train}
        \begin{lstlisting}[caption=学習時の入力層の処理,label=fuga]
        def input_layer_train(X, j):
            batch_index = create_batch(X)
            input_images = X[batch_index[j]] / 255
            image_size = 784
            class_num = 10
            input_vector = input_images.reshape(100,image_size)
            return input_vector, image_size, batch_index, class_num
        \end{lstlisting}
        学習時の入力層の処理を行う関数である.
            \subparagraph*{入力}
            \begin{itemize}
                \item X: 元の画像データ
                \item j: batch\_indexの何行目を参照するかを指定するint
            \end{itemize}
            入力Xを引数にcreate\_batch関数を実行し, batch\_indexのj行目に対応するXを正規化しinput\_imagesに格納し, これをbatch\_size*image\_sizeの行列に変形する.

    \subsubsection*{6. input\_layer\_test}
        \begin{lstlisting}[caption=テストの入力層の処理,label=fuga]
        def input_layer_test(X, i):
            input_image = X[i] / 255
            image_size = input_image.size
            image_num = len(X)
            class_num = 10
            input_vector = input_image.reshape(1,image_size)
            return input_vector, image_size, i, class_num
        \end{lstlisting}
        テスト時の入力層の処理を行う関数である.
            \subparagraph*{入力}
            \begin{itemize}
                \item X: 元の画像データ
                \item i: 画像データの何番目を参照するかを指定するint
            \end{itemize}
            入力Xのi番目のデータを正規化しベクトルに変換する.

    \subsubsection*{7. matrix\_operationクラス}
        \begin{lstlisting}[caption=線形和の計算 ,label=fuga]
        class matrix_operation:
            def __init__(self, W, b):
                self.W = W
                self.b = b
                self.X = None

            def forward(self, X):
                self.X = X
                y = np.dot(X, self.W) + self.b
                return y

            def backward(self, back):
                dX = np.dot(back, self.W.T)
                dW = np.dot(self.X.T, back)
                db = np.sum(back, axis=0)
                return dX, dW, db
        \end{lstlisting}
        線形和の計算に関するクラスである.
        \paragraph*{\_\_init\_\_(コンストラクタ)}
            \subparagraph*{入力}
            \begin{itemize}
                \item W: 重み
                \item b: バイアス
            \end{itemize}
            \subparagraph*{メンバ変数}
            \begin{itemize}
                \item W: 重み
                \item b: バイアス
                \item X: 順伝播の入力
            \end{itemize}
        \paragraph*{forward関数}
            \subparagraph*{入力}
            \begin{itemize}
                \item X: 順伝播の入力
            \end{itemize}
            X, W, bで線形和を計算する.
        \paragraph*{backward関数}
            \subparagraph*{入力}
            \begin{itemize}
                \item back: \(\displaystyle \frac{\partial E_n}{\partial y}\)
            \end{itemize}
            順伝播の出力yによる偏微分backを入力として受け取り, これを元に\(\displaystyle \frac{\partial E_n}{\partial X}\), \(\displaystyle \frac{\partial E_n}{\partial W}\), \(\displaystyle \frac{\partial E_n}{\partial b}\)を計算する.

    \subsubsection*{8. sigmoidクラス}
        \begin{lstlisting}[caption=シグモイド関数の計算 ,label=fuga]
        class sigmoid:
            def __init__(self):
                self.y = None

            def forward(self, t):
                self.y = (1 / (1 + np.exp(-1 * t)))
                return self.y

            def backward(self, back):
                dt = back * (1 - self.y) * self.y
                return dt
        \end{lstlisting}
        シグモイド関数に関するクラスである.
        \paragraph*{\_\_init\_\_(コンストラクタ)}
            \subparagraph*{メンバ変数}
            \begin{itemize}
                \item y: 順伝播の出力
            \end{itemize}
        \paragraph*{forward関数}
            \subparagraph*{入力}
            \begin{itemize}
                \item t: 順伝播の1つ前の演算結果
            \end{itemize}
            シグモイド関数の関数適用を行う.
        \paragraph*{backward関数}
            \subparagraph*{入力}
            \begin{itemize}
                \item back: \(\displaystyle \frac{\partial E_n}{\partial y}\)
            \end{itemize}
            順伝播の出力yによる偏微分backを入力として受け取り, これを元に\(\displaystyle \frac{\partial E_n}{\partial t}\)を計算する.

    \subsubsection*{9. ReLUクラス}
        \begin{lstlisting}[caption=ReLU関数の計算 ,label=fuga]
        class ReLU():
            def __init__(self):
                self.a = None

            def forward(self, t):
                self.a = np.where(t > 0, t, 0)
                return self.a

            def backward(self, back):
                dt = back * np.where(self.a > 0, 1, 0)
                return dt
        \end{lstlisting}
        ReLU関数に関するクラスである.
        \paragraph*{\_\_init\_\_(コンストラクタ)}
            \subparagraph*{メンバ変数}
            \begin{itemize}
                \item a: 順伝播の出力
            \end{itemize}
        \paragraph*{forward関数}
            \subparagraph*{入力}
            \begin{itemize}
                \item t: 順伝播の1つ前の演算結果
            \end{itemize}
            ReLU関数の関数適用を行う.
        \paragraph*{backward関数}
            \subparagraph*{入力}
            \begin{itemize}
                \item back: \(\displaystyle \frac{\partial E_n}{\partial a}\)
            \end{itemize}
            順伝播の出力aによる偏微分backを入力として受け取り, これを元に\(\displaystyle \frac{\partial E_n}{\partial t}\)を計算する.

    \subsubsection*{10. Dropoutクラス}
        \begin{lstlisting}[caption=Dropout関数の計算 ,label=fuga]
        def __init__(self, rho):
            self.rho = rho
            self.mask = None

        def forward(self, t, train_flag=1):
            if train_flag == 1:
                self.mask = np.random.rand(*t.shape) > self.rho
                a = t * self.mask
                return a
            else:
                a = t * (1 - self.rho)
                return  a

        def backward(self, back):
            dt = back * self.mask
            return dt
        \end{lstlisting}
        Dropout関数に関するクラスである.
        \paragraph*{\_\_init\_\_(コンストラクタ)}
            \subparagraph*{メンバ変数}
            \begin{itemize}
                \item rho: 無視するノードの割合
                \item mask: 無視しない/する要素の位置をそれぞれTrue/Falseとした行列
            \end{itemize}
        \paragraph*{forward関数}
            \subparagraph*{入力}
            \begin{itemize}
                \item t: 順伝播の1つ前の演算結果
                \item train\_flag: 順伝播が学習時かテスト時かを制御するフラグ
            \end{itemize}
            Dropout関数の関数適用を行う.
            なお, train\_flagが1のときに学習時である.
        \paragraph*{backward関数}
            \subparagraph*{入力}
            \begin{itemize}
                \item back: \(\displaystyle \frac{\partial E_n}{\partial a}\)
            \end{itemize}
            順伝播の出力aによる偏微分backを入力として受け取り, これを元に\(\displaystyle \frac{\partial E_n}{\partial t}\)を計算する.

    \subsubsection*{11. Batch\_Normalizationクラス}
        \begin{lstlisting}[caption=Batch\_Normalizationに関する計算 ,label=fuga]
        class Batch_Normalization():
            mean_list = []
            var_list = []

            def __init__(self):
                self.batch_size = None
                self.gamma = 1
                self.beta = 0
                self.x = None
                self.mean = None
                self.var = None
                self.normalized_x = None
                self.epsilon = 1e-7
                self.op1 = optimize('Adam')
                self.op2 = optimize('Adam')

            def forward(self, x, train_flag=1):
                self.x = x
                if train_flag == 1:
                    self.batch_size = x.shape[0]
                    self.mean = np.mean(x, axis=0)
                    self.var = np.var(x, axis=0)
                    # print('x: ',  x.shape)
                    self.normalized_x = (x - self.mean) / np.sqrt(self.var + self.epsilon)
                    y = self.gamma * self.normalized_x + self.beta
                else:
                    y = self.gamma / np.sqrt(np.mean(Batch_Normalization.var_list, axis=0) + self.epsilon) * x + \
                        (self.beta - self.gamma * np.mean(Batch_Normalization.mean_list, axis=0) / np.sqrt(np.mean(Batch_Normalization.var_list, axis=0) + self.epsilon))
                return y

            def backward(self, back):
                dn_x = back * self.gamma
                dvar = np.sum(dn_x * (self.x - self.mean) * (-1 / 2) * (self.var + self.epsilon) ** (-3 / 2), axis=0)
                dmean = np.sum(dn_x * (-1) / np.sqrt(self.var + self.epsilon), axis=0) + dvar * np.sum(-2 * (self.x - self.mean), axis=0) / self.batch_size
                dx = dn_x / np.sqrt(self.var + self.epsilon) + dvar * 2 * (self.x - self.mean) / self.batch_size + dmean / self.batch_size
                dgamma = np.sum(back * self.normalized_x, axis=0)
                dbeta = np.sum(back, axis=0)
                self.gamma += self.op1.update(dgamma)
                self.beta += self.op2.update(dbeta)
                return dx
        \end{lstlisting}
        Batch\_Normalizationに関するクラスである.
            \subparagraph*{クラス変数}
            \begin{itemize}
                \item mean\_list: ミニバッチの平均を格納するためのリスト
                \item var\_list: ミニバッチの分散を格納するためのリスト
            \end{itemize}
        \paragraph*{\_\_init\_\_(コンストラクタ)}
            \subparagraph*{メンバ変数}
            \begin{itemize}
                \item batch\_size: バッチサイズ
                \item gamma: 正規化後に出力を調整するためのパラメータ
                \item beta: 正規化後に出力を調整するためのパラメータ
                \item x: 順伝播の1つ前の演算結果
                \item mean: ミニバッチの平均
                \item var: ミニバッチの分散
                \item normalized\_x: xを正規化したもの
                \item epsilon: 分母=0を防ぐための微小量
                \item op1, op2: gamma, betaの最適化を行うクラスのインスタンス
            \end{itemize}

        \paragraph*{forward関数}
            \subparagraph*{入力}
            \begin{itemize}
                \item x: 順伝播の1つ前の演算結果
                \item train\_flag: 順伝播が学習時かテスト時かを制御するフラグ
            \end{itemize}
            Batch\_Normalizationの順伝播の計算を行う.
            なお, train\_flagが1のときに学習時である.
        \paragraph*{backward関数}
            \subparagraph*{入力}
            \begin{itemize}
                \item back: \(\displaystyle \frac{\partial E_n}{\partial y}\)
            \end{itemize}
            順伝播の出力yによる偏微分backを入力として受け取り, これを元に\(\displaystyle \frac{\partial E_n}{\partial x}\), \(\displaystyle \frac{\partial E_n}{\partial gamma}\), \(\displaystyle \frac{\partial E_n}{\partial beta}\)を計算する.
            また, gammaとbetaの更新を行う.

    \subsubsection*{12. softmaxクラス}
        \begin{lstlisting}[caption=ソフトマックス関数の計算 ,label=fuga]
        class softmax:
            def __init__(self, batch_size):
                self.y_pred = None
                self.batch_size = batch_size

            def forward(self, a):
                alpha = np.tile(np.amax(a, axis=1), 10).reshape(10, self.batch_size).T
                exp_a = np.exp(a - alpha)
                sum_exp = np.tile(np.sum(exp_a, axis=1), 10).reshape(10, self.batch_size).T
                self.y_pred = exp_a / sum_exp
                return self.y_pred

            def backward(self, y_ans, B):
                da = (self.y_pred - y_ans) / B
                return da
        \end{lstlisting}
        softmax関数に関するクラスである.
        \paragraph*{\_\_init\_\_(コンストラクタ)}
            \subparagraph*{メンバ変数}
            \begin{itemize}
                \item y\_pred: 出力層の出力
                \item batch\_size: バッチサイズ
            \end{itemize}
        \paragraph*{forward関数}
            \subparagraph*{入力}
            \begin{itemize}
                \item a: 順伝播の1つ前の演算結果
            \end{itemize}
            softmax関数の関数適用を行う.
        \paragraph*{backward関数}
            \subparagraph*{入力}
            \begin{itemize}
                \item y\_ans: 正解クラスをone-hot vector表記にしたもの
                \item B: バッチサイズ
            \end{itemize}
            y\_ans, B入力として受け取り, これを元に\(\displaystyle \frac{\partial E_n}{\partial a}\)を計算する.

    \subsubsection*{13. postprocessing関数}
        \begin{lstlisting}[caption=後処理,label=fuga]
        def postprocessing(y):
            binary_y = np.where(y == np.amax(y, axis=1), 1, 0)
            return binary_y
        \end{lstlisting}
        後処理を行うための関数である.
            \subparagraph*{入力}
            \begin{itemize}
                \item y: 出力層の出力
            \end{itemize}
            yを受け取り, 行ごとに最も大きい値を1, それ以外を0にする.

    \subsubsection*{14. cross\_entropy\_loss関数}
        \begin{lstlisting}[caption=クロスエントロピー誤差の計算,label=fuga]
        def cross_entropy_loss(y_pred, y_ans):
            B = len(y_pred)
            E = 1 / B * np.sum((-1) * y_ans * np.log(y_pred))
            return E
        \end{lstlisting}
        クロスエントロピー誤差を計算するための関数である.
            \subparagraph*{入力}
            \begin{itemize}
                \item y\_pred: 出力層の出力
                \item y\_ans: 正解クラスをone-hot vector表記にしたもの
            \end{itemize}
            y\_predとy\_ansによってクロスエントロピー誤差を計算する.

    \subsubsection*{15. neural\_networkクラス}
        \begin{lstlisting}[caption=3層ニューラルネットワークの構成 ,label=fuga]
        class neural_network():
            def __init__(self, batch_size, epoch, middle_layer, last):
                self.batch_size = batch_size
                self.epoch = epoch
                self.middle_layer = middle_layer
                self.last = last

            def learning(self):
                params1 = params(self.middle_layer, 784)
                params2 = params(self.last, self.middle_layer)
                for i in range(self.epoch):
                    loss = []
                    for j in range(int(60000 / self.batch_size)):
                        input_vec, image_size, batch_index, class_sum = input_layer_train(train_X, j)
                        batch_label = train_Y[batch_index[j]]
                        y_ans = np.identity(10)[batch_label]

                        W1, b1 = params1.W, params1.b
                        mo1 = matrix_operation(W1, b1)
                        t = mo1.forward(input_vec)
                        # print('matrix', t)

                        bn = Batch_Normalization()
                        y_bn = bn.forward(t)
                        # y_bn = t

                        # sig = sigmoid()
                        # y1 = sig.forward(t)
                        re = ReLU()
                        y_re = re.forward(y_bn)
                        # print('sigmoid', y1)

                        dr = Dropout(0.2)
                        y1 = dr.forward(y_re)

                        W2, b2 = params2.W, params2.b
                        mo2 = matrix_operation(W2, b2)
                        a = mo2.forward(y1)
                        # print('a', a)
                        soft = softmax(self.batch_size)
                        y2 = soft.forward(a)
                        # print(y2)
                        # binary_y = postprocessing(y2)
                        # print(binary_y)
                        E = cross_entropy_loss(y2, y_ans)
                        loss.append(E)

                        da = soft.backward(y_ans, self.batch_size)
                        dX2, dW2, db2 = mo2.backward(da)

                        dt_dr = dr.backward(dX2)

                        # dt = sig.backward(dX2)
                        dt_re = re.backward(dt_dr)

                        dt = bn.backward(dt_re)
                        # dt = dt_re

                        dX1, dW1, db1 = mo1.backward(dt)
                        params1.update(dW1, db1)
                        params2.update(dW2, db2)
                    Batch_Normalization.mean_list = np.append(Batch_Normalization.mean_list, bn.mean, axis=0)
                    Batch_Normalization.var_list = np.append(Batch_Normalization.var_list, bn.var, axis=0)
                    # Batch_Normalization.var_list.append(bn.var)
                    print(np.sum(loss) / len(loss))

                params1.save(1)
                params2.save(2)

            def testing(self, all_flag=1):
                # input_vector, image_size, i, class_num = input_layer(test_X)
                ans = []
                Batch_Normalization.mean_list = Batch_Normalization.mean_list.reshape([self.epoch, self.middle_layer])
                Batch_Normalization.var_list = Batch_Normalization.var_list.reshape([self.epoch, self.middle_layer])
                # print(Batch_Normalization.mean_list.shape)
                if all_flag == 1:
                    for k in range(test_Y.shape[0]):
                        input_vector, image_size, i, class_num = input_layer_test(test_X, k)
                        # y_ans = np.identity(10)[test_Y[i]]
                        W1, b1 = load(1)
                        mo1 = matrix_operation(W1, b1)
                        t = mo1.forward(input_vector)

                        bn = Batch_Normalization()
                        y_bn = bn.forward(t, 0)
                        # y_bn = t

                        # print('matrix', y1)
                        # sig = sigmoid()
                        # y1 = sig.forward(t)
                        re = ReLU()
                        y_re = re.forward(y_bn)
                        # print('sigmoid', y1)

                        dr = Dropout(0.2)
                        y1 = dr.forward(y_re, 0)

                        W2, b2 = load(2)
                        mo2 = matrix_operation(W2, b2)
                        a = mo2.forward(y1)
                        # print('a', a)

                        soft = softmax(1)
                        y2 = soft.forward(a)
                        # print(y2)
                        binary_y = postprocessing(y2)
                        # print(np.where(binary_y == 1)[1][0], test_Y[i])
                        eq = 1 if np.where(binary_y == 1)[1][0] == test_Y[i] else 0
                        ans.append(eq)
                    print(np.mean(ans))
                else:
                    k = int(input('テストデータの何番目を試すか入力してください'))
                    input_vector, image_size, i, class_num = input_layer_test(test_X, k)
                    # y_ans = np.identity(10)[test_Y[i]]
                    W1, b1 = load(1)
                    mo1 = matrix_operation(W1, b1)
                    t = mo1.forward(input_vector)

                    bn = Batch_Normalization()
                    y_bn = bn.forward(t, 0)
                    # y_bn = t

                    # print('matrix', y1)
                    # sig = sigmoid()
                    # y1 = sig.forward(t)
                    re = ReLU()
                    y_re = re.forward(y_bn)
                    # print('sigmoid', y1)

                    dr = Dropout(0.2)
                    y1 = dr.forward(y_re, 0)

                    W2, b2 = load(2)
                    mo2 = matrix_operation(W2, b2)
                    a = mo2.forward(y1)
                    # print('a', a)

                    soft = softmax(1)
                    y2 = soft.forward(a)
                    # print(y2)
                    binary_y = postprocessing(y2)
                    print('予測結果: ', np.where(binary_y == 1)[1][0], '正解: ', test_Y[k])
        \end{lstlisting}
        ニューラルネットワークの構成に関するクラスである.
        \paragraph*{\_\_init\_\_(コンストラクタ)}
            \subparagraph*{メンバ変数}
            \begin{itemize}
                \item batch\_size: バッチサイズ
                \item epoch: エポック数
                \item middle\_layer: 中間層のノード数
                \item last: 出力層のノード数
            \end{itemize}
            メンバ変数それぞれを設定する.
        \paragraph*{learning関数}
            学習を行う.
            入力層$\rightarrow$
            線形和$\rightarrow$
            Batch\_Normalization$\rightarrow$
            ReLU$\rightarrow$
            Dropout$\rightarrow$
            中間層$\rightarrow$
            線形和$\rightarrow$
            出力層$\rightarrow$
            softmax($\rightarrow$クロスエントロピー誤差)という構成で
            順伝播, 逆伝播の計算を行い, パラメータを更新し,
            1エポックごとにクロスエントロピー誤差とBatch\_Normalizationの平均, 分散をそれぞれリストに格納する.
            そして, 学習終了後に重みWとバイアスbを.npyファイルとして保存する.
        \paragraph*{testing関数}
            \subparagraph*{入力}
            \begin{itemize}
                \item all\_flag: テストデータ全てを活用するか1つを活用するか制御するフラグ
            \end{itemize}
            学習時に保存した.npyファイルを読み込み, 全テストデータに関してニューラルネットワークの計算を行う.
            all\_flagが1であればテストデータの全てに関して計算を実行し, 最後に正解率を標準出力に出力する. 一方all\_flagが0であれば標準入力でテストデータの何番目を活用するか受け取り, 最後に計算による予測結果と正解を標準出力に出力する.

    \subsubsection*{学習とテストの実行}
        \begin{lstlisting}[caption=学習とテストの実行,label=fuga]
        nn = neural_network(100, 20, 50, 10)
        print('学習を開始します. ')
        nn.learning()
        print('テストを開始します. ')
        nn.testing(all_flag=0)
        \end{lstlisting}
        neural\_networkクラスをインスタンス化し, 学習とテストを実行する.
        なお, バッチサイズ: 100, エポック数: 20, 中間層のノード数: 50, 出力層のノード数: 10で実行した.

\subsection*{実行結果}
クロスエントロピー誤差が0.7203779372223761から0.05723036650842045まで減少し,
多くのテストデータについて正解を導けた. なお, all\_flag=1とし, 全てのテストデータに関して実行すると正解率は95.87\%となった.

\subsection*{工夫点}
    \begin{itemize}
        \item 機能ごとクラスを作成することで可読性と保守性を高めた.
        \item 様々な行列に関する演算を実装する際にfor文を使わず, numpyの機能を活用することで計算時間を抑えた.
    \end{itemize}
\subsection*{問題点}
    \begin{itemize}
        \item バッチサイズを一箇所で管理できていない.
    \end{itemize}
\end{document}
